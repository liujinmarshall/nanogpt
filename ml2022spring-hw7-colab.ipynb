{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T19:03:37.179026Z",
     "iopub.status.busy": "2023-07-11T19:03:37.17865Z",
     "iopub.status.idle": "2023-07-11T19:03:37.18801Z",
     "shell.execute_reply": "2023-07-11T19:03:37.186287Z",
     "shell.execute_reply.started": "2023-07-11T19:03:37.178996Z"
    },
    "id": "xvSGDbExff_I"
   },
   "source": [
    "# **Homework 7 - Bert (Question Answering)**\n",
    "\n",
    "If you have any questions, feel free to email us at mlta-2022-spring@googlegroups.com\n",
    "\n",
    "\n",
    "\n",
    "Slide:    [Link](https://docs.google.com/presentation/d/1H5ZONrb2LMOCixLY7D5_5-7LkIaXO6AGEaV2mRdTOMY/edit?usp=sharing)　Kaggle: [Link](https://www.kaggle.com/c/ml2022spring-hw7)　Data: [Link](https://drive.google.com/uc?id=1AVgZvy3VFeg0fX-6WQJMHPVrx3A-M1kb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGOr_eS3wJJf"
   },
   "source": [
    "## Task description\n",
    "- Chinese Extractive Question Answering\n",
    "  - Input: Paragraph + Question\n",
    "  - Output: Answer\n",
    "\n",
    "- Objective: Learn how to fine tune a pretrained model on downstream task using transformers\n",
    "\n",
    "- Todo\n",
    "    - Fine tune a pretrained chinese BERT model\n",
    "    - Change hyperparameters (e.g. doc_stride)\n",
    "    - Apply linear learning rate decay\n",
    "    - Try other pretrained models\n",
    "    - Improve preprocessing\n",
    "    - Improve postprocessing\n",
    "- Training tips\n",
    "    - Automatic mixed precision\n",
    "    - Gradient accumulation\n",
    "    - Ensemble\n",
    "\n",
    "- Estimated training time (tesla t4 with automatic mixed precision enabled)\n",
    "    - Simple: 8mins\n",
    "    - Medium: 8mins\n",
    "    - Strong: 25mins\n",
    "    - Boss: 2.5hrs\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJ1fSAJE2oaC"
   },
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YPrc4Eie9Yo5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 10 19:27:49 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 552.41       CUDA Version: 12.4     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        On  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   35C    P8              11W / 320W |   2008MiB / 16376MiB |     16%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1049      G   /Xwayland                                 N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Download link 1\n",
    "#!gdown --id '1AVgZvy3VFeg0fX-6WQJMHPVrx3A-M1kb' --output hw7_data.zip\n",
    "\n",
    "# Download Link 2 (if the above link fails)\n",
    "#!gdown --id '1qwjbRjq481lHsnTrrF4OjKQnxzgoLEFR' --output hw7_data.zip\n",
    "\n",
    "# Download Link 3 (if the above link fails)\n",
    "# !gdown --id '1QXuWjNRZH6DscSd6QcRER0cnxmpZvijn' --output hw7_data.zip\n",
    "\n",
    "#!unzip -o hw7_data.zip\n",
    "\n",
    "# For this HW, K80 < P4 < T4 < P100 <= T4(fp16) < V100\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TevOvhC03m0h"
   },
   "source": [
    "## Install transformers\n",
    "\n",
    "Documentation for the toolkit:　https://huggingface.co/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tbxWFX_jpDom"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (4.44.0)\n",
      "Requirement already satisfied: torch in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: matplotlib in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (3.9.1.post1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from matplotlib) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# You are allowed to change version of transformers or use other toolkits\n",
    "#!pip install transformers==4.5.0\n",
    "!pip install transformers torch\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dKM4yCh4LI_"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WOTHHtWJoahe"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW, BertForQuestionAnswering, BertTokenizerFast, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "def same_seeds(seed):\n",
    "\t  torch.manual_seed(seed)\n",
    "\t  if torch.cuda.is_available():\n",
    "\t\t    torch.cuda.manual_seed(seed)\n",
    "\t\t    torch.cuda.manual_seed_all(seed)\n",
    "\t  np.random.seed(seed)\n",
    "\t  random.seed(seed)\n",
    "\t  torch.backends.cudnn.benchmark = False\n",
    "\t  torch.backends.cudnn.deterministic = True\n",
    "same_seeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7pBtSZP1SKQO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\n",
    "fp16_training = True\n",
    "\n",
    "if fp16_training:\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\", gradient_accumulation_steps=4)\n",
    "    device = accelerator.device\n",
    "else:\n",
    "    device = device\n",
    "\n",
    "# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YgXHuVLp_6j"
   },
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xyBCYGjAp3ym"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"wptoux/albert-chinese-large-qa\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# FacebookAI/xlm-roberta-base\n",
    "# google-bert/bert-base-multilingual-cased\n",
    "# google-bert/bert-base-chinese\n",
    "# google-bert/bert-large-uncased\n",
    "# FacebookAI/roberta-large\n",
    "# ckiplab/bert-base-chinese-ner\n",
    "# wptoux/albert-chinese-large-qa\n",
    "# uer/roberta-base-chinese-extractive-qa\n",
    "\n",
    "# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Td-GTmk5OW4"
   },
   "source": [
    "## Read Data\n",
    "\n",
    "- Training set: 31690 QA pairs\n",
    "- Dev set: 4131  QA pairs\n",
    "- Test set: 4957  QA pairs\n",
    "\n",
    "- {train/dev/test}_questions:\n",
    "  - List of dicts with the following keys:\n",
    "   - id (int)\n",
    "   - paragraph_id (int)\n",
    "   - question_text (string)\n",
    "   - answer_text (string)\n",
    "   - answer_start (int)\n",
    "   - answer_end (int)\n",
    "- {train/dev/test}_paragraphs:\n",
    "  - List of strings\n",
    "  - paragraph_ids in questions correspond to indexs in paragraphs\n",
    "  - A paragraph may be used by several questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:[./]\n",
      "+--ml2022spring-hw7.zip\n",
      "+--result_preprocessing.csv\n",
      "+--result_lr_update.csv\n",
      "+--result_start_end_swap.csv\n",
      "+--saved_model\n",
      "|      +--model.safetensors\n",
      "|      +--config.json\n",
      "+--result_original.csv\n",
      "+--result.csv\n",
      "+--.ipynb_checkpoints\n",
      "|      +--ml2022spring-hw7-colab-checkpoint.ipynb\n",
      "|      +--result_start_end_swap-checkpoint.csv\n",
      "|      +--hw7_train-checkpoint.json\n",
      "|      +--result_original-checkpoint.csv\n",
      "|      +--hw7_test-checkpoint.json\n",
      "+--hw7_test.json\n",
      "+--hw7_train.json\n",
      "+--hw7_dev.json\n",
      "+--ml2022spring-hw7-colab.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "def dfs_showdir(path, depth):\n",
    "    if depth == 0:\n",
    "        print(\"root:[\" + path + \"]\")\n",
    "\n",
    "    for item in os.listdir(path):\n",
    "        if '.git' not in item:\n",
    "            print(\"|      \" * depth + \"+--\" + item)\n",
    "\n",
    "            newitem = path +'/'+ item\n",
    "            if os.path.isdir(newitem):\n",
    "                dfs_showdir(newitem, depth +1)\n",
    "\n",
    "dfs_showdir('./', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NvX7hlepogvu"
   },
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    with open(file, 'r', encoding=\"utf-8\") as reader:\n",
    "        data = json.load(reader)\n",
    "    return data[\"questions\"], data[\"paragraphs\"]\n",
    "\n",
    "# train_questions, train_paragraphs = read_data(\"hw7_train.json\")\n",
    "# dev_questions, dev_paragraphs = read_data(\"hw7_dev.json\")\n",
    "# test_questions, test_paragraphs = read_data(\"hw7_test.json\")\n",
    "train_questions, train_paragraphs = read_data(\"hw7_train.json\")\n",
    "dev_questions, dev_paragraphs = read_data(\"hw7_dev.json\")\n",
    "test_questions, test_paragraphs = read_data(\"hw7_test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fm0rpTHq0e4N"
   },
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rTZ6B70Hoxie"
   },
   "outputs": [],
   "source": [
    "# Tokenize questions and paragraphs separately\n",
    "# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__\n",
    "\n",
    "train_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n",
    "dev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n",
    "test_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False)\n",
    "\n",
    "train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\n",
    "dev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False)\n",
    "test_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False)\n",
    "\n",
    "# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1894, 2135, 7269, 4638, 7531, 4666, 677, 3298, 3300, 862, 6172, 7617, 4289, 136]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print (test_questions_tokenized.data['input_ids'][0])\n",
    "print (test_questions_tokenized.data['token_type_ids'][0])\n",
    "print (test_questions_tokenized.data['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [len(train_paragrpah) for train_paragrpah in train_paragraphs_tokenized.data['input_ids']]\n",
    "num_bins = 12\n",
    "# Create the histogram\n",
    "plt.hist(data, bins=num_bins, edgecolor='black')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Distribution Bucket Diagram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "#print((train_paragraphs_tokenized.data['input_ids'][0]))\n",
    "#print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5397, 7679, 3136, 4640, 1164, 1953, 676, 686, 1762, 8280, 2399, 3633, 2466, 1217, 1089, 6306, 4158, 5397, 7679, 782, 4638, 4640, 2370, 136]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print (train_questions_tokenized.data['input_ids'][0])\n",
    "#print (train_questions_tokenized.data['token_type_ids'][0])\n",
    "print (train_questions_tokenized.data['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(test_questions_tokenized.data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws8c8_4d5UCI"
   },
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Xjooag-Swnuh"
   },
   "outputs": [],
   "source": [
    "class QA_Dataset(Dataset):\n",
    "    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n",
    "        self.split = split\n",
    "        self.questions = questions\n",
    "        self.tokenized_questions = tokenized_questions\n",
    "        self.tokenized_paragraphs = tokenized_paragraphs\n",
    "        self.max_question_len = 40\n",
    "        self.max_paragraph_len = 150\n",
    "\n",
    "        ##### TODO: Change value of doc_stride #####\n",
    "        self.doc_stride = 75\n",
    "\n",
    "        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n",
    "        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        tokenized_question = self.tokenized_questions[idx]\n",
    "        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n",
    "\n",
    "        ##### TODO: Preprocessing #####\n",
    "        # Hint: How to prevent model from learning something it should not learn\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph\n",
    "            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n",
    "            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n",
    "\n",
    "            # A single window is obtained by slicing the portion of paragraph containing the answer\n",
    "            mid = (answer_start_token + answer_end_token) // 2\n",
    "            paragraph_start = max(0, min(mid - self.max_paragraph_len // 2, len(tokenized_paragraph) - self.max_paragraph_len))\n",
    "            paragraph_end = paragraph_start + self.max_paragraph_len\n",
    "\n",
    "            start_min = max(0, answer_end_token - self.max_paragraph_len)\n",
    "            start_max = min(answer_start_token, len(tokenized_paragraph) - self.max_paragraph_len)\n",
    "\n",
    "            import random\n",
    "            random_start = start_min + (int)(random.random() * (start_max - start_min))\n",
    "            paragraph_start = random_start\n",
    "            paragraph_end =  paragraph_start + self.max_paragraph_len\n",
    "            #print(f\"ps: {paragraph_start} , as: {answer_start_token} , ae: {answer_end_token} , pe: {paragraph_end}\")\n",
    "\n",
    "            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
    "            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\n",
    "            #print(f\"q:{len(input_ids_question)}, a:{len(input_ids_paragraph)}\")\n",
    "\n",
    "            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window\n",
    "            answer_start_token += len(input_ids_question) - paragraph_start\n",
    "            answer_end_token += len(input_ids_question) - paragraph_start\n",
    "\n",
    "            # Pad sequence and obtain inputs to model\n",
    "            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "            #print(len(input_ids))\n",
    "            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n",
    "\n",
    "        # Validation/Testing\n",
    "        else:\n",
    "            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n",
    "\n",
    "            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n",
    "            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n",
    "\n",
    "                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
    "                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n",
    "\n",
    "                # Pad sequence and obtain inputs to model\n",
    "                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "\n",
    "                input_ids_list.append(input_ids)\n",
    "                token_type_ids_list.append(token_type_ids)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "\n",
    "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n",
    "\n",
    "    def padding(self, input_ids_question, input_ids_paragraph):\n",
    "        # Pad zeros if sequence length is shorter than max_seq_len\n",
    "        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n",
    "        # Indices of input sequence tokens in the vocabulary\n",
    "        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n",
    "        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
    "        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n",
    "        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
    "        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n",
    "\n",
    "        return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n",
    "dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n",
    "test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n",
    "\n",
    "train_batch_size = 128\n",
    "\n",
    "# Note: Do NOT change batch size of dev_loader / test_loader !\n",
    "# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
    "dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_H1kqhR8CdM"
   },
   "source": [
    "## Function for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SqeA3PLPxOHu"
   },
   "outputs": [],
   "source": [
    "def evaluate(data, output):\n",
    "    ##### TODO: Postprocessing #####\n",
    "    # There is a bug and room for improvement in postprocessing\n",
    "    # Hint: Open your prediction file to see what is wrong\n",
    "\n",
    "    answer = ''\n",
    "    max_prob = float('-inf')\n",
    "    num_of_windows = data[0].shape[1]\n",
    "\n",
    "    for k in range(num_of_windows):\n",
    "        # Obtain answer by choosing the most probable start position / end position\n",
    "        start_prob, start_index = torch.max(output.start_logits[k], dim=0)\n",
    "        end_prob, end_index = torch.max(output.end_logits[k], dim=0)\n",
    "\n",
    "        # Probability of answer is calculated as sum of start_prob and end_prob\n",
    "        prob = start_prob + end_prob\n",
    "\n",
    "        if (start_index > end_index):\n",
    "            #pass\n",
    "            (start_index, end_index) = (end_index, start_index) # swap start and end if start > end\n",
    "\n",
    "        # Replace answer if calculated probability is larger than previous windows\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            # Convert tokens to chars (e.g. [1920, 7032] --> \"大 金\")\n",
    "            answer = tokenizer.decode(data[0][0][k][start_index : end_index + 1])\n",
    "\n",
    "    # Remove spaces in answer (e.g. \"大 金\" --> \"大金\")\n",
    "    return answer.replace(' ','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzHQit6eMnKG"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3Q-B6ka7xoCM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuji/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training ...\n",
      "<class 'torch.device'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a1f374cbac451f9d1627d587348a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     output\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 52\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/accelerate/optimizer.py:157\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_patched_step_method\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called:\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;66;03m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/torch/amp/grad_scaler.py:454\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    452\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 454\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "num_epoch = 1\n",
    "validation = True\n",
    "logging_step = 100\n",
    "learning_rate = 1e-4\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_data_size = train_set.__len__()\n",
    "t_total = train_data_size // train_batch_size * num_epoch  # Total number of training steps\n",
    "warmup_steps = 0.1 * t_total  # Warmup steps as a fraction of total training steps\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    ")\n",
    "\n",
    "if fp16_training:\n",
    "    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Start Training ...\")\n",
    "print(type(device))\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    step = 1\n",
    "    train_loss = train_acc = 0\n",
    "\n",
    "    for data in tqdm(train_loader):\n",
    "        # Load all data into GPU\n",
    "        data = [i.to(device) for i in data]\n",
    "\n",
    "        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n",
    "        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)\n",
    "        #with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n",
    "\n",
    "        # Choose the most probable start position / end position\n",
    "        start_index = torch.argmax(output.start_logits, dim=1)\n",
    "        end_index = torch.argmax(output.end_logits, dim=1)\n",
    "\n",
    "        # Prediction is correct only if both start_index and end_index are correct\n",
    "        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n",
    "        train_loss += output.loss\n",
    "\n",
    "        if fp16_training:\n",
    "            accelerator.backward(output.loss)\n",
    "        else:\n",
    "            output.loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        step += 1\n",
    "\n",
    "        ##### TODO: Apply linear learning rate decay #####\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        lr -= lr / step / step\n",
    "        if (lr < 0):\n",
    "            lr = -lr\n",
    "        #optimizer.param_groups[0]['lr'] = lr\n",
    "        if (step % 10 == 0):\n",
    "            pass\n",
    "            #print(lr)\n",
    "\n",
    "        \n",
    "\n",
    "        # Print training loss and accuracy over past logging step\n",
    "        if step % logging_step == 0:\n",
    "            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}\")\n",
    "            train_loss = train_acc = 0\n",
    "\n",
    "    if validation:\n",
    "        print(\"Evaluating Dev Set ...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dev_acc = 0\n",
    "            for i, data in enumerate(tqdm(dev_loader)):\n",
    "                #with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "                # prediction is correct only if answer text exactly matches\n",
    "                dev_acc += evaluate(data, output) == dev_questions[i][\"answer_text\"]\n",
    "            print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n",
    "        model.train()\n",
    "\n",
    "# Save a model and its configuration file to the directory 「saved_model」\n",
    "# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n",
    "# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\n",
    "print(\"Saving Model ...\")\n",
    "model_save_dir = \"saved_model\"\n",
    "model.save_pretrained(model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMmdLOKBMsdE"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "U5scNKC9xz0C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Test Set ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28677757384e40d282e11636d2e26516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4957 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed! Result is in result.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating Test Set ...\")\n",
    "\n",
    "result = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader):\n",
    "        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "        result.append(evaluate(data, output))\n",
    "\n",
    "result_file = \"result.csv\"\n",
    "with open(result_file, 'w') as f:\n",
    "\t  f.write(\"ID,Answer\\n\")\n",
    "\t  for i, test_question in enumerate(test_questions):\n",
    "        # Replace commas in answers with empty strings (since csv is separated by comma)\n",
    "        # Answers in kaggle are processed in the same way\n",
    "\t\t    f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n",
    "\n",
    "print(f\"Completed! Result is in {result_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 3493170,
     "sourceId": 35999,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30528,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
